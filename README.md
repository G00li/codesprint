# CodeSprint - Plataforma de Desenvolvimento de Projetos

## üîç Sobre o Projeto

O **CodeSprint** √© uma plataforma inovadora que utiliza Intelig√™ncia Artificial para acelerar o desenvolvimento de novos projetos. Com uma interface intuitiva e responsiva, desenvolvedores podem especificar suas necessidades e obter um projeto base completo em quest√£o de minutos.

## üõ†Ô∏è Arquitetura do Sistema

O CodeSprint √© composto por v√°rios servi√ßos interconectados:

- **Frontend**: Interface de usu√°rio React/Next.js (porta 3000)
- **Backend**: API REST em Python/FastAPI (porta 8000)
- **CrewAI**: Servi√ßo de orquestra√ß√£o de IA (porta 8004)
- **Ollama**: Servi√ßo de execu√ß√£o de modelos de linguagem (porta 11434)
- **Redis**: Cache e filas de tarefas (porta 6379)

Os servi√ßos se comunicam entre si atrav√©s de uma rede Docker dedicada.

## üìã Requisitos

- Docker e Docker Compose
- 8GB+ de RAM dispon√≠vel
- 20GB+ de espa√ßo em disco
- Conex√£o est√°vel com a internet (para download inicial dos modelos)

## üöÄ Instala√ß√£o e Execu√ß√£o

### Instala√ß√£o R√°pida

```bash
# Clone o reposit√≥rio
git clone https://github.com/seu-usuario/codesprint.git
cd codesprint

# Inicie os servi√ßos (com download autom√°tico do modelo LLM)
./start_services.sh
```

### Usando Makefile

```bash
# Inicia todos os cont√™ineres
make up

# Para todos os cont√™ineres
make down

# Reinicia todos os cont√™ineres
make restart

# Limpa ambiente (remove cont√™ineres e volumes)
make clean
```

### Verifica√ß√£o da Instala√ß√£o

```bash
# Verifica o status dos servi√ßos
./check_services.sh
```

## üîß Ferramentas de Diagn√≥stico

O CodeSprint inclui diversas ferramentas para diagn√≥stico de problemas de conectividade entre os servi√ßos.

### 1. Diagn√≥stico via Interface Web

Acesse as ferramentas de diagn√≥stico pela interface web:

- **Diagn√≥stico Autom√°tico**: http://localhost:3000/diagnostico
- **Diagn√≥stico Manual**: http://localhost:3000/diagnostico/manual

A ferramenta de diagn√≥stico manual permite testar a conectividade com qualquer endpoint, facilitando a identifica√ß√£o de problemas espec√≠ficos.

### 2. Script de Diagn√≥stico

Execute o script de diagn√≥stico para verificar todos os servi√ßos:

```bash
./check_services.sh
```

Este script verifica:
- Status dos cont√™ineres Docker
- Conectividade com cada servi√ßo
- Logs de servi√ßos com problemas
- Diagn√≥stico de rede interno

### 3. Endpoints de Diagn√≥stico no Backend

O backend exp√µe endpoints espec√≠ficos para diagn√≥stico:

- **Health Check**: http://localhost:8000/health
- **Diagn√≥stico CrewAI**: http://localhost:8000/diagnose-crewai
- **Diagn√≥stico de Rede**: http://localhost:8000/diagnose-network

### 4. Script de Corre√ß√£o de Conectividade

Para corrigir problemas comuns de conectividade:

```bash
./fix_connection.sh
```

Este script detecta e corrige automaticamente problemas de configura√ß√£o entre os servi√ßos.

### 5. Comandos Docker √öteis

Verifique o status e os logs dos servi√ßos:

```bash
# Verificar status dos cont√™ineres
docker-compose ps

# Ver logs do servi√ßo backend
docker-compose logs backend

# Ver logs do servi√ßo crewai
docker-compose logs crewai

# Ver logs do servi√ßo ollama
docker-compose logs ollama

# Executar diagn√≥stico de rede interno
docker-compose exec backend python -m app.services.network_diagnostics
```

## üîÑ Conectividade entre Cont√™ineres

Ao usar o sistema em cont√™ineres Docker, lembre-se que:

1. **Nomes de servi√ßos vs Localhost**: 
   - Use `backend`, `crewai` e `ollama` como nomes de hosts dentro da rede Docker
   - Use `localhost` apenas quando acessar os servi√ßos de fora dos cont√™ineres

2. **Exemplo de URLs**:
   - Dentro dos cont√™ineres: `http://backend:8000/health`
   - Acesso externo: `http://localhost:8000/health`

3. **Vari√°veis de ambiente**:
   - A vari√°vel `NEXT_PUBLIC_BACKEND_URL` define a URL do backend
   - O valor padr√£o nos cont√™ineres √© `http://backend:8000`
   - Ao rodar localmente, defina como `http://localhost:8000`

4. **Testando conectividade**:
   - Use a ferramenta de diagn√≥stico manual em http://localhost:3000/diagnostico/manual
   - Ao testar dentro dos cont√™ineres, use os nomes dos servi√ßos (`backend`, `crewai`, etc.)

## üö® Resolu√ß√£o de Problemas Comuns

### Erro de Conex√£o Recusada (Connection Refused)

Isso geralmente indica que o servi√ßo n√£o est√° rodando ou n√£o est√° acess√≠vel na porta configurada.

**Solu√ß√£o**: Verifique se o cont√™iner est√° em execu√ß√£o com `docker-compose ps` e analise os logs com `docker-compose logs [servi√ßo]`.

### Erro 500 do CrewAI

Se o backend consegue se comunicar com o CrewAI, mas recebe um erro 500, geralmente √© um problema com o modelo do Ollama.

**Solu√ß√£o**: Verifique se o Ollama est√° rodando e se o modelo especificado est√° dispon√≠vel:

```bash
# Verificar status do Ollama
docker-compose logs ollama

# Verificar a comunica√ß√£o Ollama-CrewAI
docker-compose exec backend python -m app.test_crewai_connection
```

### Timeout em Requisi√ß√µes

Timeouts podem ocorrer quando o Ollama est√° carregando o modelo pela primeira vez, o que pode levar alguns minutos.

**Solu√ß√£o**: Aguarde alguns minutos ap√≥s iniciar os servi√ßos e tente novamente. Verifique os logs do Ollama para acompanhar o progresso.

### Erro "Failed to fetch" no Frontend

Esse erro geralmente ocorre quando o frontend tenta acessar o backend usando `localhost` em vez do nome do servi√ßo Docker.

**Solu√ß√£o**: 
1. Verifique se a configura√ß√£o da URL do backend est√° correta no frontend
2. Use o diagn√≥stico manual para testar com a URL correta (`http://backend:8000/...`)
3. Reinicie o cont√™iner frontend ap√≥s modificar vari√°veis de ambiente: `docker-compose restart frontend`

## üìä Monitoramento e Desempenho

### Verifica√ß√£o de Recursos

Para monitorar o uso de recursos pelos cont√™ineres:

```bash
docker stats
```

### Verifica√ß√£o de Modelos Ollama

Para verificar os modelos dispon√≠veis no Ollama:

```bash
docker exec -it codesprint-ollama-1 ollama list
```

### Download Manual de Modelos

Para baixar manualmente o modelo LLM usado pelo sistema:

```bash
docker exec -it codesprint-ollama-1 ollama pull llama3:8b
```

## üìö Documenta√ß√£o Adicional

Consulte estes arquivos para informa√ß√µes espec√≠ficas:

- `CHANGELOG.md`: Hist√≥rico de altera√ß√µes e melhorias no sistema
- `backend/README_TROUBLESHOOTING.md`: Guia detalhado de solu√ß√£o de problemas

## üß™ Desenvolvimento e Contribui√ß√£o

Para contribuir com o projeto:

1. Escolha uma issue aberta ou crie uma nova
2. Fa√ßa fork do reposit√≥rio
3. Crie um branch para sua feature (`git checkout -b feature/nome-da-feature`)
4. Implemente suas altera√ß√µes
5. Execute os testes necess√°rios
6. Fa√ßa commit das altera√ß√µes (`git commit -am 'Adiciona nova feature'`)
7. Fa√ßa push para o branch (`git push origin feature/nome-da-feature`)
8. Abra um Pull Request

## üìÑ Licen√ßa

Desenvolvido com ‚ù§Ô∏è pelo time CodeSprint

[Incluir informa√ß√µes de licen√ßa se aplic√°vel]